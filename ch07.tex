%!TEX root = da-dev.tex

So far we have been using the model of computing that we introduced in Chapter~\ref{ch:pn}: a distributed algorithm $A$ is a state machine that whose state transitions determined by functions $\Init_{A,d}$, $\Send_{A,d}$, and $\Receive_{A,d}$. Everything has been fully deterministic: for a given port-numbered network and a fixed input, the algorithm will always produce the same output. In this chapter, we will extend the model so that we can study randomised distributed algorithms.


\section{Definitions}\label{sec:randomised}

Let us first define a \emph{randomised distributed algorithms in the $\PN$ model} or, in brief, a \emph{randomised $\PN$ algorithm}. We extend the definitions of Section~\ref{sec:distr-alg} so that the state transitions are chosen randomly according to some probability distribution that may depend on the current state and incoming messages.

More formally, the values of the functions $\Init_{A,d}$ and $\Receive_{A,d}$ are discrete probability distributions over $\States_A$. Then the initial state of a node $u$ is a random variable
\[
    x_0(u) \sim \Init_{A,d}(f(u))
\]
chosen from a discrete probability distribution $\Init_{A,d}(f(u))$ that may depend on the initial state $f(u)$, and the state at time $t$ is a random variable
\[
    x_t(u) \sim \Receive_{A,d}\bigl(x_{t-1}(u), m_t(u) \bigr).
\]
chosen from a discrete probability distribution $\Receive_{A,d}(x_{t-1}(u), m_t(u))$ that may depend on the previous state $x_{t-1}(u)$ and on the incoming messages $m_t(u)$. All other parts of the model are as before. In particular, function $\Send_{A,d}$ is deterministic.

So far we have defined randomised $\PN$ algorithms. We can now extend the definitions in a natural manner to define randomised algorithms in the $\LOCAL$ model (add unique identifiers, see Chapter~\ref{ch:local}) and randomised algorithms in the $\CONGEST$ model (add unique identifiers and limit the size of the messages, see Chapter~\ref{ch:congest}).


\section{Probabilistic Analysis}

With the introduction of randomness, the guarantees that we have typically become probabilistic. For example, we may claim that algorithm $A$ \emph{stops in time $T$ with probability $p$}.

Note that all probabilities are over the random choices that the state machines make. We do not assume that our network or the local inputs are somehow random. For example, if we claim that algorithm $A$ solves problem $\Pi$ on graph family $\calF$ in time $T(n)$ with probability $p$, then we can take \emph{any} graph $G \in \calF$ and \emph{any} port-numbered network $N$ with $G$ as its underlying graph, and we guarantee that with probability at least $p$ the execution of $A$ in $N$ stops in time $T(n)$ and produces a correct output $g \in \Pi(G)$.

We may occasionally want to emphasise the distinction between ``Monte Carlo'' and ``Las Vegas'' type algorithms:
\begin{itemize}
    \item Monte Carlo: Algorithm $A$ always stops in time $T(n)$; the output is a correct solutions to problem $\Pi$ with probability $p$.
    \item Las Vegas: Algorithm $A$ stops in time $T(n)$ with probability $p$; when it stops, the output is always a correct solutions to problem $\Pi$.
\end{itemize}
However, Monte Carlo algorithms are not as useful in the field of distributed computing as they were in the context of centralised algorithms. In centralised algorithms, we can usually take a Monte Carlo algorithm and just run it repeatedly until it produces a feasible solution; hence we can turn a Monte Carlo algorithm into a Las Vegas algorithm. This is not necessarily the case with distributed algorithms: verifying the output of an algorithm may require global information on the entire output, and gathering such information may take a long time. In this chapter, we will focus on Las Vegas algorithms that are always correct but may occasionally be slow.



